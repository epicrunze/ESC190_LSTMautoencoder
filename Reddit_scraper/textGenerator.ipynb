{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Successfully logged in to Weights & Biases!\nwandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\DS/.netrc\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/text-generator-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/text-generator-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/text-generator-esc190/runs/bgklhnax\" target=\"_blank\">https://app.wandb.ai/epicrunze/text-generator-esc190/runs/bgklhnax</a><br/>\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import os\n",
    "import utils\n",
    "import pickle\n",
    "\n",
    "!wandb login \"41c25b4fc8e96d4ae0d96e0abd4d69787a6ea35f\"\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"text-generator-esc190\")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing word dictionaries \n",
    "\n",
    "vec2word = pickle.load(open(\"normedvec2word512.p\", \"rb\"))\n",
    "word2vec = pickle.load(open(\"normedword2vec512.p\", \"rb\"))\n",
    "VECTOR_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "vec2word = {tuple(key / np.sum(key)): value.lower() for key, value in vec2word.items()}\n",
    "word2vec = {key.lower(): value[0] / np.sum(value[0]) for key, value in word2vec.items()}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#generating random embeddings for punctuation\n",
    "punc2embed = {}\n",
    "for each in utils.punc_list:\n",
    "    notnormed = np.random.randn(VECTOR_SIZE)\n",
    "    punc2embed[each] = notnormed / np.sum(notnormed)\n",
    "    vec2word[tuple(punc2embed[each])] = each\n",
    "    word2vec[each] = punc2embed[each]\n",
    "pickle.dump(punc2embed, open(\"randpunc2embed16.p\", \"wb\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pickle.dump(vec2word, open(\"normedvec2word16.p\", \"wb\"))\n",
    "pickle.dump(word2vec, open(\"normedword2vec16.p\", \"wb\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nkeys = list(word2vec.keys())\\nvalues = [word2vec[each] for each in keys]\\n\\nkeys = tf.constant(keys, dtype=tf.string)\\nvalues = tf.constant(values)\\nprint(values[0].shape)\\n\\ntf_word2vec = tf.lookup.StaticHashTable(\\n    initializer=tf.lookup.KeyValueTensorInitializer(\\n        keys=keys,\\n        values=values,\\n    ),\\n    default_value=tf.zeros(()),\\n    name=\"num2wordlookup\"\\n)\\n'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "'''\n",
    "keys = list(word2vec.keys())\n",
    "values = [word2vec[each] for each in keys]\n",
    "\n",
    "keys = tf.constant(keys, dtype=tf.string)\n",
    "values = tf.constant(values)\n",
    "print(values[0].shape)\n",
    "\n",
    "tf_word2vec = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=keys,\n",
    "        values=values,\n",
    "    ),\n",
    "    default_value=tf.zeros(()),\n",
    "    name=\"num2wordlookup\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "characterSet = []\n",
    "for each in list(word2vec.keys()):\n",
    "    characterSet.extend(list(each))\n",
    "characterSet = set(characterSet)\n",
    "characterSet = list(characterSet)\n",
    "characterSet.sort()\n",
    "print(characterSet)\n",
    "pickle.dump(characterSet, open(\"charlist.p\", \"wb\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\n#generating random embeddings for punctuation\\npunc2embed = {}\\nfor each in utils.punc_list:\\n    notnormed = np.random.randn(VECTOR_SIZE)\\n    punc2embed[each] = notnormed / np.sum(notnormed)\\npickle.dump(punc2embed, open(\"randpunc2embed.p\", \"wb\"))\\n'"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "'''\n",
    "#generating random embeddings for punctuation\n",
    "punc2embed = {}\n",
    "for each in utils.punc_list:\n",
    "    notnormed = np.random.randn(VECTOR_SIZE)\n",
    "    punc2embed[each] = notnormed / np.sum(notnormed)\n",
    "pickle.dump(punc2embed, open(\"randpunc2embed.p\", \"wb\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(inputfile, outputfile):\n",
    "    #text processing\n",
    "    charlist = pickle.load(open(\"charlist.p\", \"rb\"))\n",
    "    textfile = inputfile\n",
    "    outputfn = outputfile\n",
    "    fileyboi = open(textfile, \"r\", encoding=\"utf8\")\n",
    "    outputfile = open(outputfn, \"w\")\n",
    "\n",
    "    outputlist = []\n",
    "\n",
    "    for char in fileyboi.read():\n",
    "        char = char.lower()\n",
    "        if char in utils.punc_list or char in charlist or char == \" \":\n",
    "            outputlist.append(char)\n",
    "    outputstr = \"\".join(outputlist)\n",
    "    for each in utils.punc_list:\n",
    "        outputstr = outputstr.replace(each, \" {} \".format(each))\n",
    "\n",
    "    outputstr = outputstr.split(\" \")\n",
    "    outputstr = \" \".join(outputstr)\n",
    "\n",
    "    outputlist = []\n",
    "\n",
    "    for each in outputstr.split(\" \"):\n",
    "        if each in word2vec:\n",
    "            outputlist.append(each)\n",
    "    outputstr = \" \".join(outputlist)\n",
    "    outputfile.write(outputstr)\n",
    "    fileyboi.close()\n",
    "    outputfile.close()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processText(\"nosleep.txt\", \"processednosleep.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading text in\n",
    "#creating function to be mapped\n",
    "#uses word2vec\n",
    "def str2embeddedvec_py(string):\n",
    "    output = word2vec[string.numpy().decode(\"ascii\")]\n",
    "    return output\n",
    "\n",
    "def str2embeddedvec(string):\n",
    "    [output,] = tf.py_function(str2embeddedvec_py, inp=[string], Tout=[tf.float32])\n",
    "    output.set_shape(VECTOR_SIZE)\n",
    "    return output\n",
    "\n",
    "def split2chunks(inp):\n",
    "    input_vec = inp[:-1]\n",
    "    target_vec = inp[1:]\n",
    "    return input_vec, target_vec\n",
    "\n",
    "sequence_length = 16\n",
    "BUFFER_SIZE = 5000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening file\n",
    "filename = \"processednosleep.txt\"\n",
    "with open(filename, \"r\") as processedFile: \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(np.array(processedFile.read().split(\" \")))\n",
    "    dataset = dataset.map(str2embeddedvec)\n",
    "    dataset = dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "    dataset = dataset.map(split2chunks)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#wowies = dataset.take(1)\n",
    "#print(np.array(list(wowies.as_numpy_iterator())).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(32, 16, 32)]            0         \n_________________________________________________________________\nlstm (LSTM)                  (32, 16, 2000)            16264000  \n_________________________________________________________________\ntime_distributed (TimeDistri (32, 16, 32)              64032     \n=================================================================\nTotal params: 16,328,032\nTrainable params: 16,328,032\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "#creating model\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "\n",
    "NUM_UNITS = 2000\n",
    "inputs = Input(batch_shape=(BATCH_SIZE, sequence_length, VECTOR_SIZE))\n",
    "LSTM1 = LSTM(NUM_UNITS, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(inputs)\n",
    "output = TimeDistributed(Dense(VECTOR_SIZE))(LSTM1)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "optimizer = Adam(learning_rate = 0.0003)\n",
    "\n",
    "model.compile(loss = cosine_similarity, optimizer = optimizer, metrics=[\"cosine_similarity\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = BATCH_SIZE\n",
    "config.epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"512nosleep.1.09.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save_model = ModelCheckpoint(filepath=\"32nosleep.1.{epoch:02d}.h5\", monitor='cosine_similarity', save_weights_only=True, mode='auto', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/text-generator-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/text-generator-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/text-generator-esc190/runs/52gs8j0k\" target=\"_blank\">https://app.wandb.ai/epicrunze/text-generator-esc190/runs/52gs8j0k</a><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 2895 steps\nEpoch 1/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3030 - cosine_similarity: 0.3030\nEpoch 00001: saving model to 32nosleep.1.01.h5\n2895/2895 [==============================] - 323s 111ms/step - loss: -0.3030 - cosine_similarity: 0.3030\nEpoch 2/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3231 - cosine_similarity: 0.3231\nEpoch 00002: saving model to 32nosleep.1.02.h5\n2895/2895 [==============================] - 316s 109ms/step - loss: -0.3231 - cosine_similarity: 0.3231\nEpoch 3/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3281 - cosine_similarity: 0.3281\nEpoch 00003: saving model to 32nosleep.1.03.h5\n2895/2895 [==============================] - 321s 111ms/step - loss: -0.3281 - cosine_similarity: 0.3281\nEpoch 4/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3308 - cosine_similarity: 0.3308\nEpoch 00004: saving model to 32nosleep.1.04.h5\n2895/2895 [==============================] - 326s 112ms/step - loss: -0.3308 - cosine_similarity: 0.3308\nEpoch 5/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3333 - cosine_similarity: 0.3333\nEpoch 00005: saving model to 32nosleep.1.05.h5\n2895/2895 [==============================] - 322s 111ms/step - loss: -0.3333 - cosine_similarity: 0.3333\nEpoch 6/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3355 - cosine_similarity: 0.3355\nEpoch 00006: saving model to 32nosleep.1.06.h5\n2895/2895 [==============================] - 328s 113ms/step - loss: -0.3355 - cosine_similarity: 0.3355\nEpoch 7/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3376 - cosine_similarity: 0.3376\nEpoch 00007: saving model to 32nosleep.1.07.h5\n2895/2895 [==============================] - 340s 117ms/step - loss: -0.3376 - cosine_similarity: 0.3376\nEpoch 8/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3397 - cosine_similarity: 0.3397\nEpoch 00008: saving model to 32nosleep.1.08.h5\n2895/2895 [==============================] - 327s 113ms/step - loss: -0.3397 - cosine_similarity: 0.3397\nEpoch 9/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3423 - cosine_similarity: 0.3423\nEpoch 00009: saving model to 32nosleep.1.09.h5\n2895/2895 [==============================] - 338s 117ms/step - loss: -0.3423 - cosine_similarity: 0.3423\nEpoch 10/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3456 - cosine_similarity: 0.3456\nEpoch 00010: saving model to 32nosleep.1.10.h5\n2895/2895 [==============================] - 329s 114ms/step - loss: -0.3456 - cosine_similarity: 0.3456\nEpoch 11/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3497 - cosine_similarity: 0.3497\nEpoch 00011: saving model to 32nosleep.1.11.h5\n2895/2895 [==============================] - 329s 113ms/step - loss: -0.3496 - cosine_similarity: 0.3496\nEpoch 12/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3547 - cosine_similarity: 0.3547\nEpoch 00012: saving model to 32nosleep.1.12.h5\n2895/2895 [==============================] - 305s 106ms/step - loss: -0.3547 - cosine_similarity: 0.3547\nEpoch 13/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3611 - cosine_similarity: 0.3611\nEpoch 00013: saving model to 32nosleep.1.13.h5\n2895/2895 [==============================] - 336s 116ms/step - loss: -0.3611 - cosine_similarity: 0.3611\nEpoch 14/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3683 - cosine_similarity: 0.3683\nEpoch 00014: saving model to 32nosleep.1.14.h5\n2895/2895 [==============================] - 330s 114ms/step - loss: -0.3683 - cosine_similarity: 0.3683\nEpoch 15/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3762 - cosine_similarity: 0.3762\nEpoch 00015: saving model to 32nosleep.1.15.h5\n2895/2895 [==============================] - 320s 111ms/step - loss: -0.3762 - cosine_similarity: 0.3762\nEpoch 16/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3844 - cosine_similarity: 0.3844\nEpoch 00016: saving model to 32nosleep.1.16.h5\n2895/2895 [==============================] - 333s 115ms/step - loss: -0.3844 - cosine_similarity: 0.3844\nEpoch 17/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.3930 - cosine_similarity: 0.3930\nEpoch 00017: saving model to 32nosleep.1.17.h5\n2895/2895 [==============================] - 339s 117ms/step - loss: -0.3930 - cosine_similarity: 0.3930\nEpoch 18/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.4013 - cosine_similarity: 0.4013\nEpoch 00018: saving model to 32nosleep.1.18.h5\n2895/2895 [==============================] - 335s 116ms/step - loss: -0.4013 - cosine_similarity: 0.4013\nEpoch 19/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.4100 - cosine_similarity: 0.4100\nEpoch 00019: saving model to 32nosleep.1.19.h5\n2895/2895 [==============================] - 312s 108ms/step - loss: -0.4100 - cosine_similarity: 0.4100\nEpoch 20/20\n2894/2895 [============================>.] - ETA: 0s - loss: -0.4183 - cosine_similarity: 0.4183\nEpoch 00020: saving model to 32nosleep.1.20.h5\n2895/2895 [==============================] - 312s 108ms/step - loss: -0.4183 - cosine_similarity: 0.4183\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x22c2c9be188>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "wandb.init()\n",
    "model.fit(dataset, epochs=config.epochs, callbacks=[WandbCallback(), save_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def vec2predictions(inputvec, word_arr, vec_arr, temperature = 0.005, prob = True, num_samples = 1):\n",
    "    '''\n",
    "    inputvec => np.array of shape (variable, VECTOR_SIZE)\n",
    "    word_arr => np.array of shape (vocab_size, 1)\n",
    "    vec_arr => np.array of shape (vocab_size, VECTOR_SIZE)\n",
    "    returns a np.array of shape (variable), with words\n",
    "    '''\n",
    "    dotted_prod = inputvec @ vec_arr.T\n",
    "    norm_vec_arr = LA.norm(vec_arr, axis=1)\n",
    "    norm_inputvec = LA.norm(inputvec, axis=1)\n",
    "    divisor = np.expand_dims(norm_inputvec.T, 1) @ np.expand_dims(norm_vec_arr, 0)\n",
    "    '''\n",
    "    print(\"computed divisor with shape\", divisor.shape)\n",
    "    print(\"computed dotted_prod with shape\", dotted_prod.shape)\n",
    "    '''\n",
    "    finalvec = dotted_prod / divisor\n",
    "    finalvec = finalvec / temperature\n",
    "\n",
    "    if prob:\n",
    "        indicies = tf.random.categorical(finalvec, num_samples=num_samples)\n",
    "    else:\n",
    "        indicies = np.argsort(-finalvec)[:num_samples]\n",
    "    indicies = tf.squeeze(indicies).numpy()\n",
    "\n",
    "    #print(\"computed final prediction vector: with shape\", indicies.shape)\n",
    "\n",
    "    output = word_arr[indicies]\n",
    "    return output\n",
    "\n",
    "def processString(inputstr, punc_list, charlist, word2vec):\n",
    "    '''\n",
    "    returns a list of fully processed strings with guarenteed mappings with alpha num chars and punctuation\n",
    "    '''\n",
    "    outputlist = []\n",
    "    for char in inputstr:\n",
    "        char = char.lower()\n",
    "        if char in punc_list or char in charlist or char == \" \":\n",
    "            outputlist.append(char)\n",
    "\n",
    "    outputstr = \"\".join(outputlist)\n",
    "\n",
    "    for each in punc_list:\n",
    "        outputstr = outputstr.replace(each, \" {} \".format(each))\n",
    "\n",
    "    outputstr = outputstr.split(\" \")\n",
    "    outputstr = \" \".join(outputstr)\n",
    "\n",
    "    outputlist = []\n",
    "\n",
    "    for each in outputstr.split(\" \"):\n",
    "        if each in word2vec:\n",
    "            outputlist.append(each)\n",
    "    \n",
    "    return outputlist\n",
    "\n",
    "\n",
    "def generateText(model, start_string, word2vec, punc_list, char_list, num_generate = 15, temperature = 0.005):\n",
    "    word_arr = []\n",
    "    vec_arr = []\n",
    "    for word, vector in word2vec.items():\n",
    "        word_arr.append(word)\n",
    "        vec_arr.append(vector)\n",
    "    word_arr = np.array(word_arr)\n",
    "    vec_arr = np.array(vec_arr)\n",
    "\n",
    "    inputvec = []\n",
    "    for each in processString(start_string, punc_list, char_list, word2vec):\n",
    "        inputvec.append(word2vec[each])\n",
    "\n",
    "    inputvec = np.array(inputvec)\n",
    "\n",
    "    inputvec = tf.expand_dims(inputvec, 0)\n",
    "\n",
    "    outputText = []\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        #print(\"starting prediction\")\n",
    "        pred = model.predict(inputvec)\n",
    "        #print(\"finished prediction: output shape = \", pred.shape)\n",
    "        # divide prediction by temperature to do stuff\n",
    "        pred = tf.expand_dims(pred[:, -1, :], 0)\n",
    "        pred = tf.squeeze(pred, 0)\n",
    "        predicted_word = vec2predictions(pred, word_arr, vec_arr, temperature)\n",
    "        #print(\"predicted word is: \", predicted_word)\n",
    "        outputText.append(predicted_word)\n",
    "\n",
    "        inputvec = tf.expand_dims(tf.expand_dims(word2vec[predicted_word], 0), 0)\n",
    "\n",
    "    return (start_string + \" \".join(outputText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(1, None, 512)]          0         \n_________________________________________________________________\nlstm (LSTM)                  (1, None, 2000)           20104000  \n_________________________________________________________________\ntime_distributed (TimeDistri (None, None, 512)         1024512   \n=================================================================\nTotal params: 21,128,512\nTrainable params: 21,128,512\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "\n",
    "NUM_UNITS = 2000\n",
    "inputs = Input(batch_shape=(1, None, VECTOR_SIZE))\n",
    "LSTM1 = LSTM(NUM_UNITS, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(inputs)\n",
    "output = TimeDistributed(Dense(VECTOR_SIZE))(LSTM1)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "optimizer = Adam(learning_rate = 0.0003)\n",
    "\n",
    "model.compile(loss = cosine_similarity, optimizer = optimizer, metrics=[\"cosine_similarity\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"512nosleep.2.10.h5\")\n",
    "#model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"512generatormodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "remember i there are two odd things about i live. ganoin overlordship stun_gun stun_gun . \n \n speakership egg_timer scrubber thaneship egg_timer stun_gun rope_yarn egg_timer . \n \n entlebucher egg_timer riddle , movement stun_gun egg_timer . \n \n speakership egg_timer trucking_company thaneship . ganoin supreme_allied_commander_europe supreme_allied_commander_europe . \n \n aleph hypocritical riddle thaneship airline delta_ray . \n \n speakership\n"
    }
   ],
   "source": [
    "charlist = pickle.load(open(\"charlist.p\", \"rb\"))\n",
    "print(generateText(model, \"remember i there are two odd things about i live\", word2vec, utils.punc_list, charlist, num_generate=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}