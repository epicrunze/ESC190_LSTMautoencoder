diff --git a/bin, but it's python/__pycache__/structs.cpython-37.pyc b/bin, but it's python/__pycache__/structs.cpython-37.pyc
index 852d7fe..12d1a5d 100644
Binary files a/bin, but it's python/__pycache__/structs.cpython-37.pyc and b/bin, but it's python/__pycache__/structs.cpython-37.pyc differ
diff --git a/bin, but it's python/__pycache__/utils.cpython-37.pyc b/bin, but it's python/__pycache__/utils.cpython-37.pyc
index 1d7180a..c4d5daf 100644
Binary files a/bin, but it's python/__pycache__/utils.cpython-37.pyc and b/bin, but it's python/__pycache__/utils.cpython-37.pyc differ
diff --git a/bin, but it's python/main.ipynb b/bin, but it's python/main.ipynb
index 2720081..756d316 100644
--- a/bin, but it's python/main.ipynb	
+++ b/bin, but it's python/main.ipynb	
@@ -8,7 +8,7 @@
     "name": "ipython",
     "version": 3
    },
-   "version": "3.7.5-final"
+   "version": "3.7.4-final"
   },
   "orig_nbformat": 2,
   "file_extension": ".py",
@@ -18,8 +18,8 @@
   "pygments_lexer": "ipython3",
   "version": 3,
   "kernelspec": {
-   "name": "python37564bitprojenvvenv64a22ff99f0e49308299b7eb2e75ef79",
-   "display_name": "Python 3.7.5 64-bit ('projenv': venv)"
+   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
+   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
   }
  },
  "cells": [
@@ -37,47 +37,30 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 11,
    "metadata": {},
    "outputs": [],
-   "source": [
-    "def process_line(string):\n",
-    "    '''takes in a line of the format: <word> <def>; <def>;...\n",
-    "    returns words: str, defs: list\n",
-    "    '''\n",
-    "    defs = []\n",
-    "    splitted = string.split()\n",
-    "    word = splitted.pop(0)\n",
-    "    for each in (\" \".join(splitted)).split(\";\"):\n",
-    "        defs.append(each.strip())\n",
-    "    return word, defs"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": "['half-length', 'wowie', 'half-length', 'wowie', 'half-length', 'wowie']\n"
-    }
-   ],
    "source": [
     "datafile = \"memes.txt\"\n",
-    "dictionary = None\n",
-    "with open(datafile, \"r\") as input_doc:\n",
-    "    for line in input_doc:\n",
-    "        word, defs = process_line(line)\n",
-    "        #process defs\n",
-    "        embeddings = defs\n",
-    "        nodeyboi = structs.Node(word, defs, embeddings)\n",
-    "        if not dictionary:\n",
-    "            dictionary = structs.Dictionary(nodeyboi)\n",
-    "            continue\n",
-    "        dictionary.balanced_insert(nodeyboi)\n",
-    "print(dictionary.return_dict())"
+    "\n",
+    "def data2dict(datafile):\n",
+    "    dictionary = None\n",
+    "    with open(datafile, \"r\") as input_doc:\n",
+    "        for line in input_doc:\n",
+    "            word, defs = utils.process_line(line)\n",
+    "            #process defs\n",
+    "            embeddings = defs\n",
+    "            nodeyboi = structs.Node(word, defs, embeddings)\n",
+    "            if not dictionary:\n",
+    "                dictionary = structs.Dictionary(nodeyboi)\n",
+    "                continue\n",
+    "            dictionary.balanced_insert(nodeyboi)\n",
+    "    return dictionary\n",
+    "\n",
+    "def dict2data(dictionary, writefile):\n",
+    "    \n",
+    "\n",
+    "dictionary = data2dict(datafile)"
    ]
   },
   {
diff --git a/bin, but it's python/structs.py b/bin, but it's python/structs.py
index 64de9ae..258f606 100644
--- a/bin, but it's python/structs.py	
+++ b/bin, but it's python/structs.py	
@@ -1,7 +1,7 @@
 import numpy as np
 
 class Node:
-    def __init__(self, word: str, definition: list, encoded_vec: "list(np.array)", left = None, right= None, parent= None, height: int = 1, bf: int = 0):
+    def __init__(self, word: str, definition: list, encoded_vec, left = None, right= None, parent= None, height: int = 1, bf: int = 0):
         self.word = word
         self.definition = definition
         self.vec = encoded_vec
@@ -142,10 +142,14 @@ class Dictionary:
         else:
             return True
 
-    def return_dict(self, curr="yeet", listy=[]):
+    def return_dict(self, curr="yeet"):
         curr = curr if curr != "yeet" else self.root
+        listy = []
         if curr:
-            listy = self.return_dict(curr.left, listy)
+            listy.extend(self.return_dict(curr.left))
             listy.append(curr.word)
-            listy = self.return_dict(curr.right, listy)
-        return listy
\ No newline at end of file
+            listy.extend(self.return_dict(curr.right))
+        return listy
+
+    def write2file(self, filename):
+        pass
\ No newline at end of file
diff --git a/bin, but it's python/utils.py b/bin, but it's python/utils.py
index e69de29..ce51497 100644
--- a/bin, but it's python/utils.py	
+++ b/bin, but it's python/utils.py	
@@ -0,0 +1,13 @@
+
+
+
+def process_line(string):
+    '''takes in a line of the format: <word> <def>; <def>;...
+    returns words: str, defs: list
+    '''
+    defs = []
+    splitted = string.split()
+    word = splitted.pop(0)
+    for each in (" ".join(splitted)).split(";"):
+        defs.append(each.strip())
+    return word, defs
\ No newline at end of file
diff --git a/dataProcessing.ipynb b/dataProcessing.ipynb
index 6eff425..9fd6964 100644
--- a/dataProcessing.ipynb
+++ b/dataProcessing.ipynb
@@ -169,14 +169,7 @@
    "execution_count": 12,
    "metadata": {},
    "outputs": [],
-   "source": [
-    "def to_one_hot(x):\n",
-    "    vocab_size = 46948 + 1\n",
-    "    #output = tf.zeros((x.shape)+(vocab_size,))\n",
-    "    #mask = np.array(x) > 0\n",
-    "    label = tf.one_hot(x, vocab_size)\n",
-    "    return x, label[:, 1:]"
-   ]
+   "source": []
   },
   {
    "cell_type": "code",
@@ -198,7 +191,7 @@
     "x_train = x_train.astype(\"int32\")\n",
     "np.random.shuffle(x_train)\n",
     "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
-    "dataset = dataset.map(to_one_hot)\n",
+    "dataset = dataset.map(utils.to_one_hot)\n",
     "dataset = dataset.shuffle(1000).batch(config.batch_size)"
    ]
   },
diff --git a/universalSentenceEmbedding.ipynb b/universalSentenceEmbedding.ipynb
index 635e2c4..c60ea13 100644
--- a/universalSentenceEmbedding.ipynb
+++ b/universalSentenceEmbedding.ipynb
@@ -37,7 +37,7 @@
      "output_type": "display_data",
      "data": {
       "text/plain": "<IPython.core.display.HTML object>",
-      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190/runs/38jpvgr8\" target=\"_blank\">https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190/runs/38jpvgr8</a><br/>\n            "
+      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190/runs/ppkv0w6g\" target=\"_blank\">https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190/runs/ppkv0w6g</a><br/>\n            "
      },
      "metadata": {}
     }
@@ -61,7 +61,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [
     {
@@ -82,7 +82,6 @@
     "word2num, num2word = utils.get_word_dicts(definitions)\n",
     "\n",
     "num2word[0] = \"\"\n",
-    "\n",
     "vocab_size = len(list(word2num.keys()))\n",
     "\n",
     "print(\"Size of definition vocabulary: {}\".format(vocab_size))\n",
@@ -91,13 +90,49 @@
     "\n",
     "x_train = utils.defs_to_np(def_vectors, max_length)\n",
     "\n",
-    "print(x_train.shape)"
+    "print(x_train.shape)\n",
+    "vocab_size += 1"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 3,
    "metadata": {},
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 54, 512)]         0         \n_________________________________________________________________\nmasking (Masking)            (None, 54, 512)           0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 54, 32)            69760     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 54, 64)            24832     \n_________________________________________________________________\ntime_distributed (TimeDistri (None, 54, 100)           6500      \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 54, 46949)         4741849   \n=================================================================\nTotal params: 4,842,941\nTrainable params: 4,842,941\nNon-trainable params: 0\n_________________________________________________________________\n"
+    }
+   ],
+   "source": [
+    "#creating decoder model, taking in embedded strings, and calculating a resultant from them\n",
+    "\n",
+    "from tensorflow.keras.models import Model, Sequential\n",
+    "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, RepeatVector, TimeDistributed, Lambda, Masking\n",
+    "from tensorflow.keras.optimizers import Adam\n",
+    "from tensorflow.keras.losses import categorical_crossentropy\n",
+    "\n",
+    "\n",
+    "inputlayer = Input(shape=(max_length, 512))\n",
+    "masking_layer = Masking()(inputlayer)\n",
+    "decodingLSTM1 = LSTM(32, return_sequences=True)(masking_layer)\n",
+    "decodingLSTM2 = LSTM(64, return_sequences=True)(decodingLSTM1)\n",
+    "denseboi = TimeDistributed(Dense(100, activation=\"relu\"))(decodingLSTM2)\n",
+    "finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(denseboi)\n",
+    "output = finalDense\n",
+    "\n",
+    "model = Model(inputs=inputlayer, outputs=output)\n",
+    "optimizer = Adam(learning_rate = 0.0003)\n",
+    "model.compile(loss = categorical_crossentropy, optimizer = optimizer, metrics = [\"accuracy\"])\n",
+    "\n",
+    "model.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
    "outputs": [],
    "source": [
     "import tensorflow_hub as hub\n",
@@ -107,7 +142,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 22,
+   "execution_count": 5,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -115,84 +150,414 @@
     "from functools import partial\n",
     "\n",
     "def parse_defs(definition_vec, model, num2word, vocab_size):\n",
-    "    vocab_size = vocab_size + 1\n",
-    "    definition_string = tf.strings.join(tf.map_fn(lambda x: num2word[x], definition_vec))\n",
-    "    embedded_tens = model([definition_string])\n",
+    "    converted_tens = num2word.lookup(definition_vec)\n",
+    "    definition_string = tf.strings.join(tf.split(converted_tens, num_or_size_splits=converted_tens.shape[0], axis = 0))\n",
+    "    embedded_tens = model(definition_string)\n",
+    "    embedded_tens = tf.reshape(embedded_tens, shape=(1, 512))\n",
+    "    embedded_tens = tf.tile(embedded_tens, [definition_vec.shape[0], 1])\n",
+    "    bool_mask_tens = tf.reshape(tf.where(definition_vec > 0, 1.0, 0.0), (definition_vec.shape[0], 1))\n",
+    "    embedded_tens = embedded_tens * bool_mask_tens\n",
     "    label = tf.one_hot(definition_vec, vocab_size)\n",
-    "    return embedded_tens, label[:, 1:]\n",
+    "    return embedded_tens, label\n",
     "\n",
     ""
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# creating lookup dictionary\n",
+    "\n",
+    "keys = list(num2word.keys())\n",
+    "values = [num2word[each] for each in keys]\n",
+    "\n",
+    "tf_num2word = tf.lookup.StaticHashTable(\n",
+    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
+    "        keys=tf.constant(keys),\n",
+    "        values=tf.constant(values, dtype=tf.string),\n",
+    "    ),\n",
+    "    default_value=tf.constant(\"\"),\n",
+    "    name=\"num2wordlookup\"\n",
+    ")"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": 7,
    "metadata": {},
+   "outputs": [],
+   "source": [
+    "config.batch_size = 16\n",
+    "#config.steps_per_epoch = 100\n",
+    "config.epochs = 8"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
    "outputs": [
     {
      "output_type": "stream",
      "name": "stdout",
-     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 512)]             0         \n_________________________________________________________________\nrepeat_vector (RepeatVector) (None, 54, 512)           0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 54, 32)            69760     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 54, 64)            24832     \n_________________________________________________________________\ntime_distributed (TimeDistri (None, 54, 100)           6500      \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 54, 46948)         4741748   \n=================================================================\nTotal params: 4,842,840\nTrainable params: 4,842,840\nNon-trainable params: 0\n_________________________________________________________________\n"
+     "text": "46949\n"
     }
    ],
    "source": [
-    "#creating decoder model, taking in embedded strings, and calculating a resultant from them\n",
-    "\n",
-    "from tensorflow.keras.models import Model, Sequential\n",
-    "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, RepeatVector, TimeDistributed, Lambda\n",
-    "from tensorflow.keras.optimizers import Adam\n",
-    "from tensorflow.keras.losses import categorical_crossentropy\n",
-    "\n",
-    "\n",
-    "inputlayer = Input(shape=(512,))\n",
-    "repeatlayer = RepeatVector(max_length)(inputlayer)\n",
-    "decodingLSTM1 = LSTM(32, return_sequences=True)(repeatlayer)\n",
-    "decodingLSTM2 = LSTM(64, return_sequences=True)(decodingLSTM1)\n",
-    "denseboi = TimeDistributed(Dense(100, activation=\"relu\"))(decodingLSTM2)\n",
-    "finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(denseboi)\n",
-    "output = finalDense\n",
-    "\n",
-    "model = Model(inputs=inputlayer, outputs=output)\n",
-    "optimizer = Adam(learning_rate = 0.0003)\n",
-    "model.compile(loss = categorical_crossentropy, optimizer = optimizer, metrics = [\"accuracy\"])\n",
-    "\n",
-    "model.summary()"
+    "#creating dataset\n",
+    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
+    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
+    "print(vocab_size)\n",
+    "dataset = dataset.map(partial(parse_defs, model=embed, num2word=tf_num2word, vocab_size=vocab_size))\n",
+    "dataset = dataset.shuffle(1000).batch(config.batch_size)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model.load_weights(\"USED.FIXED1.weights.08.h5\")"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 23,
+   "execution_count": 10,
    "metadata": {},
+   "outputs": [],
+   "source": [
+    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
+    "save_model = ModelCheckpoint(filepath=\"USED.FIXED2.weights.{epoch:02d}.h5\", monitor='accuracy', save_weights_only=True, mode='auto', verbose=1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {
+    "tags": [
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend"
+    ]
+   },
    "outputs": [
+    {
+     "output_type": "display_data",
+     "data": {
+      "text/plain": "<IPython.core.display.HTML object>",
+      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190/runs/s5creisn\" target=\"_blank\">https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190/runs/s5creisn</a><br/>\n            "
+     },
+     "metadata": {}
+    },
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": "Train for 8498 steps\nEpoch 1/8\n8497/8498 [============================>.] - ETA: 0s - loss: 1.0588 - accuracy: 0.1024\nEpoch 00001: saving model to USED.FIXED1.weights.01.h5\n8498/8498 [==============================] - 4845s 570ms/step - loss: 1.0588 - accuracy: 0.1024\nEpoch 2/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.9919 - accuracy: 0.1085\nEpoch 00002: saving model to USED.FIXED1.weights.02.h5\n8498/8498 [==============================] - 4916s 579ms/step - loss: 0.9919 - accuracy: 0.1085\nEpoch 3/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.9692 - accuracy: 0.1117\nEpoch 00003: saving model to USED.FIXED1.weights.03.h5\n8498/8498 [==============================] - 8387s 987ms/step - loss: 0.9692 - accuracy: 0.1117\nEpoch 4/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.9556 - accuracy: 0.1141\nEpoch 00004: saving model to USED.FIXED1.weights.04.h5\n8498/8498 [==============================] - 6682s 786ms/step - loss: 0.9556 - accuracy: 0.1141\nEpoch 5/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.9488 - accuracy: 0.1149\nEpoch 00005: saving model to USED.FIXED1.weights.05.h5\n8498/8498 [==============================] - 7451s 877ms/step - loss: 0.9487 - accuracy: 0.1149\nEpoch 6/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.9431 - accuracy: 0.1155\nEpoch 00006: saving model to USED.FIXED1.weights.06.h5\n8498/8498 [==============================] - 5234s 616ms/step - loss: 0.9431 - accuracy: 0.1155\nEpoch 7/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.9388 - accuracy: 0.1161\nEpoch 00007: saving model to USED.FIXED1.weights.07.h5\n8498/8498 [==============================] - 5005s 589ms/step - loss: 0.9387 - accuracy: 0.1161\nEpoch 8/8\n  12/8498 [..............................] - ETA: 5:57:50 - loss: 0.7840 - accuracy: 0.0747\nEpoch 00008: saving model to USED.FIXED1.weights.08.h5\n  12/8498 [..............................] - ETA: 6:01:47 - loss: 0.7840 - accuracy: 0.0747"
+    },
     {
      "output_type": "error",
-     "ename": "TypeError",
-     "evalue": "in converted code:\n\n    <ipython-input-22-066751b03f5f>:6 parse_defs  *\n        definition_string = tf.strings.join(tf.map_fn(lambda x: num2word[x], definition_vec))\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py:268 map_fn\n        maximum_iterations=n)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py:2675 while_loop\n        back_prop=back_prop)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py:194 while_loop\n        add_control_dependencies=add_control_dependencies)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:978 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py:172 wrapped_body\n        outputs = body(*_pack_sequence_as(orig_loop_vars, args))\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py:257 compute\n        packed_fn_values = fn(packed_values)\n    C:\\Users\\DS\\AppData\\Local\\Temp\\tmp3nk2ssnl.py:11 <lambda>\n        definition_string = ag__.converted_call(tf.strings.join, (ag__.converted_call(tf.map_fn, (lambda x: num2word[x], definition_vec), None, fscope),), None, fscope)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:705 __hash__\n        raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\n\n    TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\n",
+     "ename": "KeyboardInterrupt",
+     "evalue": "",
      "traceback": [
       "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
-      "\u001b[1;32m<ipython-input-23-cbaf6daccc2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://tfhub.dev/google/universal-sentence-encoder/4\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparse_defs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#dataset = dataset.shuffle(1000).batch(config.batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     \"\"\"\n\u001b[0;32m   1587\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m       return ParallelMapDataset(\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3887\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3888\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3889\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0;32m   3890\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3139\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3140\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-22-066751b03f5f>:6 parse_defs  *\n        definition_string = tf.strings.join(tf.map_fn(lambda x: num2word[x], definition_vec))\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py:268 map_fn\n        maximum_iterations=n)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py:2675 while_loop\n        back_prop=back_prop)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py:194 while_loop\n        add_control_dependencies=add_control_dependencies)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:978 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py:172 wrapped_body\n        outputs = body(*_pack_sequence_as(orig_loop_vars, args))\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py:257 compute\n        packed_fn_values = fn(packed_values)\n    C:\\Users\\DS\\AppData\\Local\\Temp\\tmp3nk2ssnl.py:11 <lambda>\n        definition_string = ag__.converted_call(tf.strings.join, (ag__.converted_call(tf.map_fn, (lambda x: num2word[x], definition_vec), None, fscope),), None, fscope)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:705 __hash__\n        raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\n\n    TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\n"
+      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
+      "\u001b[1;32m<ipython-input-11-26e685e66b76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mWandbCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\wandb\\keras\\__init__.py\u001b[0m in \u001b[0;36mnew_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
      ]
     }
    ],
    "source": [
-    "#creating dataset\n",
-    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
-    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
-    "dataset = dataset.map(partial(parse_defs, model=embed, num2word=num2word, vocab_size=vocab_size))\n",
-    "#dataset = dataset.shuffle(1000).batch(config.batch_size)"
+    "wandb.init()\n",
+    "model.fit(dataset, epochs=config.epochs, callbacks=[WandbCallback(), save_model])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def output2string(words, num2word):\n",
+    "    stringboi = \"\"\n",
+    "    for sentence in words:\n",
+    "        for word in sentence:\n",
+    "            stringboi += num2word[word]\n",
+    "            stringboi += \" \"\n",
+    "        stringboi += \"\\n\"\n",
+    "    return stringboi"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {
+    "tags": [
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend"
+    ]
+   },
+   "outputs": [],
+   "source": [
+    "for x, y in dataset:\n",
+    "    words = tf.keras.backend.argmax(y, axis=-1)\n",
+    "    words = words.numpy().tolist()\n",
+    "    print(output2string([words], num2word))"
    ]
   },
   {
