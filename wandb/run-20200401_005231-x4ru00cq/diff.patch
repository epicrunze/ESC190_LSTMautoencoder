diff --git a/universalSentenceEmbedding.ipynb b/universalSentenceEmbedding.ipynb
index 635e2c4..f539ae0 100644
--- a/universalSentenceEmbedding.ipynb
+++ b/universalSentenceEmbedding.ipynb
@@ -61,7 +61,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 75,
    "metadata": {},
    "outputs": [
     {
@@ -91,7 +91,8 @@
     "\n",
     "x_train = utils.defs_to_np(def_vectors, max_length)\n",
     "\n",
-    "print(x_train.shape)"
+    "print(x_train.shape)\n",
+    "vocab_size += 1"
    ]
   },
   {
@@ -107,7 +108,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 22,
+   "execution_count": 76,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -115,24 +116,24 @@
     "from functools import partial\n",
     "\n",
     "def parse_defs(definition_vec, model, num2word, vocab_size):\n",
-    "    vocab_size = vocab_size + 1\n",
-    "    definition_string = tf.strings.join(tf.map_fn(lambda x: num2word[x], definition_vec))\n",
-    "    embedded_tens = model([definition_string])\n",
+    "    converted_tens = num2word.lookup(definition_vec)\n",
+    "    definition_string = tf.strings.join(tf.split(converted_tens, num_or_size_splits=converted_tens.shape[0], axis = 0))\n",
+    "    embedded_tens = model(definition_string)\n",
     "    label = tf.one_hot(definition_vec, vocab_size)\n",
-    "    return embedded_tens, label[:, 1:]\n",
+    "    return embedded_tens, label\n",
     "\n",
     ""
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 77,
    "metadata": {},
    "outputs": [
     {
      "output_type": "stream",
      "name": "stdout",
-     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 512)]             0         \n_________________________________________________________________\nrepeat_vector (RepeatVector) (None, 54, 512)           0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 54, 32)            69760     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 54, 64)            24832     \n_________________________________________________________________\ntime_distributed (TimeDistri (None, 54, 100)           6500      \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 54, 46948)         4741748   \n=================================================================\nTotal params: 4,842,840\nTrainable params: 4,842,840\nNon-trainable params: 0\n_________________________________________________________________\n"
+     "text": "Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 512)]             0         \n_________________________________________________________________\nrepeat_vector_1 (RepeatVecto (None, 54, 512)           0         \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 54, 32)            69760     \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 54, 64)            24832     \n_________________________________________________________________\ntime_distributed_2 (TimeDist (None, 54, 100)           6500      \n_________________________________________________________________\ntime_distributed_3 (TimeDist (None, 54, 46949)         4741849   \n=================================================================\nTotal params: 4,842,941\nTrainable params: 4,842,941\nNon-trainable params: 0\n_________________________________________________________________\n"
     }
    ],
    "source": [
@@ -161,38 +162,96 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 23,
+   "execution_count": 78,
    "metadata": {},
+   "outputs": [],
+   "source": [
+    "# creating lookup dictionary\n",
+    "tf_num2word = tf.lookup.StaticHashTable(\n",
+    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
+    "        keys=tf.constant(list(num2word.keys())),\n",
+    "        values=tf.constant(list(num2word.values()), dtype=tf.string),\n",
+    "    ),\n",
+    "    default_value=tf.constant(\"\"),\n",
+    "    name=\"num2wordlookup\"\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 79,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#creating dataset\n",
+    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
+    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
+    "dataset = dataset.map(partial(parse_defs, model=embed, num2word=tf_num2word, vocab_size=vocab_size))\n",
+    "#dataset = dataset.shuffle(1000).batch(config.batch_size)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 80,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def output2string(words, num2word):\n",
+    "    stringboi = \"\"\n",
+    "    for sentence in words:\n",
+    "        for word in sentence:\n",
+    "            stringboi += num2word[word]\n",
+    "            stringboi += \" \"\n",
+    "        stringboi += \"\\n\"\n",
+    "    return stringboi"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 81,
+   "metadata": {
+    "tags": [
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend",
+     "outputPrepend"
+    ]
+   },
    "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": "ing having having having having having having having \n\ntaking an born senses condensing necessary abstract having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfacing imaginary skill belonging having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nimmediate do experience actual an things events skill treated having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ntaking an born fact some present having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nquantity associated self-denial abundance having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nonly associated affording having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nabundant organ supply number having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nquantities associated obtained skill abundance having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nlittle force effort sacrifice skill often illegally little produced having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ngrowing skill extreme associated abounding affording having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nlot having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nthe relation normal an having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nthe skill born than one necessary\"long skill brains\" side lush having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nexisting supply having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nonly associated self-denial obtained skill abundance having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nthe relation unchecked all-embracing growth profusely having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nextreme most having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nabundantly existing supply having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfilled living force muscles pouring events having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nverdure associated affording having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nconcepts condensing affording an deficient having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ncompared associated abundance skill obtained demand force necessary widely having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfacing distributed affected having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nscarcity condensing expensive all-embracing borrow do subjected having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ncruel do treatment exhibiting having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nsymptoms resulting repeated axis emotional forces all-embracing injury physically having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfacing abused properly having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfact worthy having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nacceptance an satisfactory skill acceptable having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nbank do skill time relation completely having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nexception bank having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfacing with do reproach skill objectionable having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfacing it's having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nthat necessary or can't can accept\" welcome having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfacing bank having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfacing liable having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nobjection do debate skill used having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nmight an not can reached large reproach do having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ntaking an born easily having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\napproached attained having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ntaking an born easily skill easy having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nreach do difficulty having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ntaking an born easily mind force self-denial all skill facing time inaccessible having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nsparsely all-embracing populated pathways having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ncompromising inaccessibly having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nlocated situated skill difficult having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nattain do difficulty skill helpful having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nabout associated together harmonious relation adaptation showing having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\ncheerful relation willingness favors do something others practicing accommodating having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nfacing intentionally having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nunaccommodating conforming having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\nexactly almost skill standard almost do present skill do relation performing skill accuracy force and marked having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having having \n\n"
+    },
     {
      "output_type": "error",
-     "ename": "TypeError",
-     "evalue": "in converted code:\n\n    <ipython-input-22-066751b03f5f>:6 parse_defs  *\n        definition_string = tf.strings.join(tf.map_fn(lambda x: num2word[x], definition_vec))\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py:268 map_fn\n        maximum_iterations=n)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py:2675 while_loop\n        back_prop=back_prop)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py:194 while_loop\n        add_control_dependencies=add_control_dependencies)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:978 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py:172 wrapped_body\n        outputs = body(*_pack_sequence_as(orig_loop_vars, args))\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py:257 compute\n        packed_fn_values = fn(packed_values)\n    C:\\Users\\DS\\AppData\\Local\\Temp\\tmp3nk2ssnl.py:11 <lambda>\n        definition_string = ag__.converted_call(tf.strings.join, (ag__.converted_call(tf.map_fn, (lambda x: num2word[x], definition_vec), None, fscope),), None, fscope)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:705 __hash__\n        raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\n\n    TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\n",
+     "ename": "KeyboardInterrupt",
+     "evalue": "",
      "traceback": [
       "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
-      "\u001b[1;32m<ipython-input-23-cbaf6daccc2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://tfhub.dev/google/universal-sentence-encoder/4\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparse_defs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#dataset = dataset.shuffle(1000).batch(config.batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     \"\"\"\n\u001b[0;32m   1587\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m       return ParallelMapDataset(\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3887\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3888\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3889\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0;32m   3890\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3139\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3140\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-22-066751b03f5f>:6 parse_defs  *\n        definition_string = tf.strings.join(tf.map_fn(lambda x: num2word[x], definition_vec))\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py:268 map_fn\n        maximum_iterations=n)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py:2675 while_loop\n        back_prop=back_prop)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py:194 while_loop\n        add_control_dependencies=add_control_dependencies)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:978 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py:172 wrapped_body\n        outputs = body(*_pack_sequence_as(orig_loop_vars, args))\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py:257 compute\n        packed_fn_values = fn(packed_values)\n    C:\\Users\\DS\\AppData\\Local\\Temp\\tmp3nk2ssnl.py:11 <lambda>\n        definition_string = ag__.converted_call(tf.strings.join, (ag__.converted_call(tf.map_fn, (lambda x: num2word[x], definition_vec), None, fscope),), None, fscope)\n    c:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:705 __hash__\n        raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\n\n    TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\n"
+      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
+      "\u001b[1;32m<ipython-input-81-dc2e74c068d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput2string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   2133\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m   \"\"\"\n\u001b[1;32m-> 2135\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(input, axis, name, dimension, output_type)\u001b[0m\n\u001b[0;32m    139\u001b[0m   axis = deprecation.deprecated_argument_lookup(\"axis\", axis, \"dimension\",\n\u001b[0;32m    140\u001b[0m                                                 dimension)\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0margmax_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36margmax_v2\u001b[1;34m(input, axis, output_type, name)\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36marg_max\u001b[1;34m(input, dimension, output_type, name)\u001b[0m\n\u001b[0;32m    831\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m    832\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ArgMax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m         tld.op_callbacks, input, dimension, \"output_type\", output_type)\n\u001b[0m\u001b[0;32m    834\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
      ]
     }
    ],
    "source": [
-    "#creating dataset\n",
-    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
-    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
-    "dataset = dataset.map(partial(parse_defs, model=embed, num2word=num2word, vocab_size=vocab_size))\n",
-    "#dataset = dataset.shuffle(1000).batch(config.batch_size)"
+    "for x, y in dataset:\n",
+    "    words = tf.keras.backend.argmax(y, axis=-1) + 1\n",
+    "    words = words.numpy().tolist()\n",
+    "    print(output2string([words], num2word))"
    ]
   },
   {
