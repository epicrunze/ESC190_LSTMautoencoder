diff --git a/__pycache__/utils.cpython-37.pyc b/__pycache__/utils.cpython-37.pyc
index e78db15..9d19ba7 100644
Binary files a/__pycache__/utils.cpython-37.pyc and b/__pycache__/utils.cpython-37.pyc differ
diff --git a/bin, but it's python/__pycache__/structs.cpython-37.pyc b/bin, but it's python/__pycache__/structs.cpython-37.pyc
index cc4806e..852d7fe 100644
Binary files a/bin, but it's python/__pycache__/structs.cpython-37.pyc and b/bin, but it's python/__pycache__/structs.cpython-37.pyc differ
diff --git a/bin, but it's python/__pycache__/utils.cpython-37.pyc b/bin, but it's python/__pycache__/utils.cpython-37.pyc
index ec32701..1d7180a 100644
Binary files a/bin, but it's python/__pycache__/utils.cpython-37.pyc and b/bin, but it's python/__pycache__/utils.cpython-37.pyc differ
diff --git a/dataProcessing.ipynb b/dataProcessing.ipynb
index d9e550f..6eff425 100644
--- a/dataProcessing.ipynb
+++ b/dataProcessing.ipynb
@@ -1,27 +1,4 @@
 {
- "nbformat": 4,
- "nbformat_minor": 2,
- "metadata": {
-  "language_info": {
-   "name": "python",
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "version": "3.7.4-final"
-  },
-  "orig_nbformat": 2,
-  "file_extension": ".py",
-  "mimetype": "text/x-python",
-  "name": "python",
-  "npconvert_exporter": "python",
-  "pygments_lexer": "ipython3",
-  "version": 3,
-  "kernelspec": {
-   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
-   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
-  }
- },
  "cells": [
   {
    "cell_type": "code",
@@ -29,17 +6,17 @@
    "metadata": {},
    "outputs": [
     {
-     "name": "stdout",
      "output_type": "stream",
+     "name": "stdout",
      "text": "Successfully logged in to Weights & Biases!\nwandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\DS/.netrc\n"
     },
     {
+     "output_type": "display_data",
      "data": {
-      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/f2utsmym\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/f2utsmym</a><br/>\n            ",
-      "text/plain": "<IPython.core.display.HTML object>"
+      "text/plain": "<IPython.core.display.HTML object>",
+      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/zxs20v9o\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/zxs20v9o</a><br/>\n            "
      },
-     "metadata": {},
-     "output_type": "display_data"
+     "metadata": {}
     }
    ],
    "source": [
@@ -55,7 +32,8 @@
     "import wandb\n",
     "from wandb.keras import WandbCallback\n",
     "wandb.init(project=\"lstm-autoencoder-esc190\")\n",
-    "config = wandb.config\n"
+    "config = wandb.config\n",
+    ""
    ]
   },
   {
@@ -64,8 +42,8 @@
    "metadata": {},
    "outputs": [
     {
-     "name": "stdout",
      "output_type": "stream",
+     "name": "stdout",
      "text": "Size of vocabulary: 117659\nLongest definition (words): 54\nNumber of definitions: 135959\nSize of definition vocabulary: 46948\n(135959, 54)\n"
     }
    ],
@@ -92,21 +70,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 3,
    "metadata": {},
-   "outputs": [
-    {
-     "ename": "NameError",
-     "evalue": "name 'vocab_size' is not defined",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
-      "\u001b[1;32m<ipython-input-1-2ddc14c4efd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mencodingLSTM1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "from tensorflow.keras.models import Model, Sequential\n",
     "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, RepeatVector, TimeDistributed, Lambda\n",
@@ -173,15 +139,20 @@
     "denseboi = TimeDistributed(Dense(100, activation=\"relu\"))(decodingLSTM2)\n",
     "finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(denseboi)\n",
     "output = finalDense\n",
-    "#creating optimizers and loss\n",
-    "\n"
+    "#creating optimizers and loss"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 4,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 64)     3004736     input_1[0][0]                    \n__________________________________________________________________________________________________\ntf_op_layer_NotEqual (TensorFlo [(None, None)]       0           input_1[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, None, 64)     33024       embedding[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 32)           12416       lstm[0][0]                       \n__________________________________________________________________________________________________\nrepeat_vector (RepeatVector)    (None, 54, 32)       0           lstm_1[0][0]                     \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   (None, 54, 32)       8320        repeat_vector[0][0]              \n__________________________________________________________________________________________________\nlstm_3 (LSTM)                   (None, 54, 64)       24832       lstm_2[0][0]                     \n__________________________________________________________________________________________________\ntime_distributed (TimeDistribut (None, 54, 100)      6500        lstm_3[0][0]                     \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 54, 46948)    4741748     time_distributed[0][0]           \n==================================================================================================\nTotal params: 7,831,576\nTrainable params: 7,831,576\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
+    }
+   ],
    "source": [
     "model = Model(inputs=inputs, outputs=output)\n",
     "optimizer = Adam(learning_rate = 0.0003)\n",
@@ -257,20 +228,28 @@
    "outputs": [
     {
      "data": {
-      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/l6c7og3x\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/l6c7og3x</a><br/>\n            ",
-      "text/plain": "<IPython.core.display.HTML object>"
+      "text/html": [
+       "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/l6c7og3x\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/l6c7og3x</a><br/>\n            "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
      },
      "metadata": {},
-     "output_type": "display_data"
+     "output_type": "execute_result"
     },
     {
      "name": "stdout",
      "output_type": "stream",
-     "text": "Train for 8498 steps\nEpoch 1/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.4075 - accuracy: 0.5190\nEpoch 00001: saving model to 32vec.5.weights.01.h5\n8498/8498 [==============================] - 2478s 292ms/step - loss: 0.4075 - accuracy: 0.5190\nEpoch 2/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.5259\nEpoch 00002: saving model to 32vec.5.weights.02.h5\n8498/8498 [==============================] - 2362s 278ms/step - loss: 0.3949 - accuracy: 0.5259\nEpoch 3/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.5304\nEpoch 00003: saving model to 32vec.5.weights.03.h5\n8498/8498 [==============================] - 2356s 277ms/step - loss: 0.3877 - accuracy: 0.5304\nEpoch 4/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.5357\nEpoch 00004: saving model to 32vec.5.weights.04.h5\n8498/8498 [==============================] - 2365s 278ms/step - loss: 0.3809 - accuracy: 0.5357\nEpoch 5/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.5396\nEpoch 00005: saving model to 32vec.5.weights.05.h5\n8498/8498 [==============================] - 2356s 277ms/step - loss: 0.3756 - accuracy: 0.5396\nEpoch 6/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.5443\nEpoch 00006: saving model to 32vec.5.weights.06.h5\n8498/8498 [==============================] - 2361s 278ms/step - loss: 0.3694 - accuracy: 0.5443\nEpoch 7/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.5485\nEpoch 00007: saving model to 32vec.5.weights.07.h5\n8498/8498 [==============================] - 2362s 278ms/step - loss: 0.3644 - accuracy: 0.5485\nEpoch 8/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.5527\nEpoch 00008: saving model to 32vec.5.weights.08.h5\n8498/8498 [==============================] - 2354s 277ms/step - loss: 0.3597 - accuracy: 0.5527\n"
+     "text": [
+      "Train for 8498 steps\nEpoch 1/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.4075 - accuracy: 0.5190\nEpoch 00001: saving model to 32vec.5.weights.01.h5\n8498/8498 [==============================] - 2478s 292ms/step - loss: 0.4075 - accuracy: 0.5190\nEpoch 2/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.5259\nEpoch 00002: saving model to 32vec.5.weights.02.h5\n8498/8498 [==============================] - 2362s 278ms/step - loss: 0.3949 - accuracy: 0.5259\nEpoch 3/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.5304\nEpoch 00003: saving model to 32vec.5.weights.03.h5\n8498/8498 [==============================] - 2356s 277ms/step - loss: 0.3877 - accuracy: 0.5304\nEpoch 4/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.5357\nEpoch 00004: saving model to 32vec.5.weights.04.h5\n8498/8498 [==============================] - 2365s 278ms/step - loss: 0.3809 - accuracy: 0.5357\nEpoch 5/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.5396\nEpoch 00005: saving model to 32vec.5.weights.05.h5\n8498/8498 [==============================] - 2356s 277ms/step - loss: 0.3756 - accuracy: 0.5396\nEpoch 6/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.5443\nEpoch 00006: saving model to 32vec.5.weights.06.h5\n8498/8498 [==============================] - 2361s 278ms/step - loss: 0.3694 - accuracy: 0.5443\nEpoch 7/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.5485\nEpoch 00007: saving model to 32vec.5.weights.07.h5\n8498/8498 [==============================] - 2362s 278ms/step - loss: 0.3644 - accuracy: 0.5485\nEpoch 8/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.5527\nEpoch 00008: saving model to 32vec.5.weights.08.h5\n8498/8498 [==============================] - 2354s 277ms/step - loss: 0.3597 - accuracy: 0.5527\n"
+     ]
     },
     {
      "data": {
-      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x24340e51c48>"
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x24340e51c48>"
+      ]
      },
      "execution_count": 17,
      "metadata": {},
@@ -298,5 +277,28 @@
    "outputs": [],
    "source": []
   }
- ]
+ ],
+ "metadata": {
+  "language_info": {
+   "name": "python",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "version": "3.7.4-final"
+  },
+  "orig_nbformat": 2,
+  "file_extension": ".py",
+  "mimetype": "text/x-python",
+  "name": "python",
+  "npconvert_exporter": "python",
+  "pygments_lexer": "ipython3",
+  "version": 3,
+  "kernelspec": {
+   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
+   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
 }
\ No newline at end of file
diff --git a/universalSentenceEmbedding.ipynb b/universalSentenceEmbedding.ipynb
index 1a8c1ab..7a44488 100644
--- a/universalSentenceEmbedding.ipynb
+++ b/universalSentenceEmbedding.ipynb
@@ -7,7 +7,8 @@
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
-   }
+   },
+   "version": "3.7.4-final"
   },
   "orig_nbformat": 2,
   "file_extension": ".py",
@@ -15,7 +16,11 @@
   "name": "python",
   "npconvert_exporter": "python",
   "pygments_lexer": "ipython3",
-  "version": 3
+  "version": 3,
+  "kernelspec": {
+   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
+   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
+  }
  },
  "cells": [
   {
@@ -35,8 +40,9 @@
     "!wandb login \"41c25b4fc8e96d4ae0d96e0abd4d69787a6ea35f\"\n",
     "import wandb\n",
     "from wandb.keras import WandbCallback\n",
-    "wandb.init(project=\"lstm-autoencoder-esc190\")\n",
-    "config = wandb.config\n"
+    "wandb.init(project=\"USE+decoder-esc190\")\n",
+    "config = wandb.config\n",
+    ""
    ]
   },
   {
@@ -64,6 +70,112 @@
     "\n",
     "print(x_train.shape)"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tensorflow_hub as hub\n",
+    "#creating embeddings from definitions\n",
+    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#calculating embeddings, in tensor format\n",
+    "embeddings = embed([utils.defs2str(definitions)[0]])\n",
+    "print(embeddings)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tensorflow_hub as hub\n",
+    "def definitions2dataset_function(model_url, num2word, vocab_size):\n",
+    "    #creating embeddings from definitions\n",
+    "    embed = hub.load(model_url)\n",
+    "    def out(definition_vec):\n",
+    "        vocab_size += 1\n",
+    "        definition_string = \" \".join([num2word[num] for num in definition_vec])\n",
+    "        embedded_tens = embed([definition_string])\n",
+    "        label = tf.one_hot(definition_vec, vocab_size)\n",
+    "        return embedded_tens, label[:, 1:]\n",
+    "    return out"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#creating decoder model, taking in embedded strings, and calculating a resultant from them\n",
+    "\n",
+    "from tensorflow.keras.models import Model, Sequential\n",
+    "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, RepeatVector, TimeDistributed, Lambda\n",
+    "from tensorflow.keras.optimizers import Adam\n",
+    "from tensorflow.keras.losses import categorical_crossentropy\n",
+    "\n",
+    "\n",
+    "inputlayer = Input(shape=(512,))\n",
+    "repeatlayer = RepeatVector(max_length)(inputlayer)\n",
+    "decodingLSTM1 = LSTM(32, return_sequences=True)(repeatlayer)\n",
+    "decodingLSTM2 = LSTM(64, return_sequences=True)(decodingLSTM1)\n",
+    "denseboi = TimeDistributed(Dense(100, activation=\"relu\"))(decodingLSTM2)\n",
+    "finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(denseboi)\n",
+    "output = finalDense\n",
+    "\n",
+    "model = Model(inputs=inputlayer, outputs=output)\n",
+    "optimizer = Adam(learning_rate = 0.0003)\n",
+    "model.compile(loss = categorical_crossentropy, optimizer = optimizer, metrics = [\"accuracy\"])\n",
+    "\n",
+    "model.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 20,
+   "metadata": {},
+   "outputs": [
+    {
+     "output_type": "error",
+     "ename": "ValueError",
+     "evalue": "Argument must be callable",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
+      "\u001b[1;32m<ipython-input-20-b098dd1561d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://tfhub.dev/google/universal-sentence-encoder/4\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefinitions2dataset_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#dataset = dataset.shuffle(1000).batch(config.batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     \"\"\"\n\u001b[0;32m   1587\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m       return ParallelMapDataset(\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3887\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3888\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3889\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0;32m   3890\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3049\u001b[0m     func_name = \"_\".join(\n\u001b[0;32m   3050\u001b[0m         [readable_transformation_name,\n\u001b[1;32m-> 3051\u001b[1;33m          function_utils.get_func_name(func)])\n\u001b[0m\u001b[0;32m   3052\u001b[0m     \u001b[1;31m# Sanitize function name to remove symbols that interfere with graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3053\u001b[0m     \u001b[1;31m# construction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\DS\\Desktop\\zhan8425_project\\projenv\\lib\\site-packages\\tensorflow_core\\python\\util\\function_utils.py\u001b[0m in \u001b[0;36mget_func_name\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m     99\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Argument must be callable'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;31mValueError\u001b[0m: Argument must be callable"
+     ]
+    }
+   ],
+   "source": [
+    "#creating dataset\n",
+    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
+    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
+    "dataset = dataset.map(definitions2dataset_function(model_url, num2word, vocab_size))\n",
+    "#dataset = dataset.shuffle(1000).batch(config.batch_size)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
   }
  ]
 }
\ No newline at end of file
diff --git a/utils.py b/utils.py
index dc83b9e..432a17b 100644
--- a/utils.py
+++ b/utils.py
@@ -1,5 +1,6 @@
 import os
 import numpy as np
+import tensorflow as tf
 
 #useful dicts and stuff
 pos_dict = {"a": "adj", "r": "adv", "n": "noun", "v": "verb"}
@@ -116,8 +117,16 @@ def defs_to_np(definitions, max_length, padding_num = 0):
     #return np.reshape(output, (output.shape + (1,))
     return output
 
+def defs2str(definitions):
+    '''converts a list of list of words to a list of strings'''
+    return [" ".join(each) for each in definitions]
 
-
+def to_one_hot(x):
+    vocab_size = 46948 + 1
+    #output = tf.zeros((x.shape)+(vocab_size,))
+    #mask = np.array(x) > 0
+    label = tf.one_hot(x, vocab_size)
+    return x, label[:, 1:]
 
 
 
