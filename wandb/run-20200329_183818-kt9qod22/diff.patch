diff --git a/__pycache__/utils.cpython-37.pyc b/__pycache__/utils.cpython-37.pyc
index e78db15..4b13a31 100644
Binary files a/__pycache__/utils.cpython-37.pyc and b/__pycache__/utils.cpython-37.pyc differ
diff --git a/bin, but it's python/__pycache__/structs.cpython-37.pyc b/bin, but it's python/__pycache__/structs.cpython-37.pyc
index cc4806e..852d7fe 100644
Binary files a/bin, but it's python/__pycache__/structs.cpython-37.pyc and b/bin, but it's python/__pycache__/structs.cpython-37.pyc differ
diff --git a/bin, but it's python/__pycache__/utils.cpython-37.pyc b/bin, but it's python/__pycache__/utils.cpython-37.pyc
index ec32701..1d7180a 100644
Binary files a/bin, but it's python/__pycache__/utils.cpython-37.pyc and b/bin, but it's python/__pycache__/utils.cpython-37.pyc differ
diff --git a/dataProcessing.ipynb b/dataProcessing.ipynb
index d9e550f..6eff425 100644
--- a/dataProcessing.ipynb
+++ b/dataProcessing.ipynb
@@ -1,27 +1,4 @@
 {
- "nbformat": 4,
- "nbformat_minor": 2,
- "metadata": {
-  "language_info": {
-   "name": "python",
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "version": "3.7.4-final"
-  },
-  "orig_nbformat": 2,
-  "file_extension": ".py",
-  "mimetype": "text/x-python",
-  "name": "python",
-  "npconvert_exporter": "python",
-  "pygments_lexer": "ipython3",
-  "version": 3,
-  "kernelspec": {
-   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
-   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
-  }
- },
  "cells": [
   {
    "cell_type": "code",
@@ -29,17 +6,17 @@
    "metadata": {},
    "outputs": [
     {
-     "name": "stdout",
      "output_type": "stream",
+     "name": "stdout",
      "text": "Successfully logged in to Weights & Biases!\nwandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\DS/.netrc\n"
     },
     {
+     "output_type": "display_data",
      "data": {
-      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/f2utsmym\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/f2utsmym</a><br/>\n            ",
-      "text/plain": "<IPython.core.display.HTML object>"
+      "text/plain": "<IPython.core.display.HTML object>",
+      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/zxs20v9o\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/zxs20v9o</a><br/>\n            "
      },
-     "metadata": {},
-     "output_type": "display_data"
+     "metadata": {}
     }
    ],
    "source": [
@@ -55,7 +32,8 @@
     "import wandb\n",
     "from wandb.keras import WandbCallback\n",
     "wandb.init(project=\"lstm-autoencoder-esc190\")\n",
-    "config = wandb.config\n"
+    "config = wandb.config\n",
+    ""
    ]
   },
   {
@@ -64,8 +42,8 @@
    "metadata": {},
    "outputs": [
     {
-     "name": "stdout",
      "output_type": "stream",
+     "name": "stdout",
      "text": "Size of vocabulary: 117659\nLongest definition (words): 54\nNumber of definitions: 135959\nSize of definition vocabulary: 46948\n(135959, 54)\n"
     }
    ],
@@ -92,21 +70,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 3,
    "metadata": {},
-   "outputs": [
-    {
-     "ename": "NameError",
-     "evalue": "name 'vocab_size' is not defined",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
-      "\u001b[1;32m<ipython-input-1-2ddc14c4efd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mencodingLSTM1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "from tensorflow.keras.models import Model, Sequential\n",
     "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, RepeatVector, TimeDistributed, Lambda\n",
@@ -173,15 +139,20 @@
     "denseboi = TimeDistributed(Dense(100, activation=\"relu\"))(decodingLSTM2)\n",
     "finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(denseboi)\n",
     "output = finalDense\n",
-    "#creating optimizers and loss\n",
-    "\n"
+    "#creating optimizers and loss"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 4,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 64)     3004736     input_1[0][0]                    \n__________________________________________________________________________________________________\ntf_op_layer_NotEqual (TensorFlo [(None, None)]       0           input_1[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, None, 64)     33024       embedding[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 32)           12416       lstm[0][0]                       \n__________________________________________________________________________________________________\nrepeat_vector (RepeatVector)    (None, 54, 32)       0           lstm_1[0][0]                     \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   (None, 54, 32)       8320        repeat_vector[0][0]              \n__________________________________________________________________________________________________\nlstm_3 (LSTM)                   (None, 54, 64)       24832       lstm_2[0][0]                     \n__________________________________________________________________________________________________\ntime_distributed (TimeDistribut (None, 54, 100)      6500        lstm_3[0][0]                     \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 54, 46948)    4741748     time_distributed[0][0]           \n==================================================================================================\nTotal params: 7,831,576\nTrainable params: 7,831,576\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
+    }
+   ],
    "source": [
     "model = Model(inputs=inputs, outputs=output)\n",
     "optimizer = Adam(learning_rate = 0.0003)\n",
@@ -257,20 +228,28 @@
    "outputs": [
     {
      "data": {
-      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/l6c7og3x\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/l6c7og3x</a><br/>\n            ",
-      "text/plain": "<IPython.core.display.HTML object>"
+      "text/html": [
+       "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/l6c7og3x\" target=\"_blank\">https://app.wandb.ai/epicrunze/lstm-autoencoder-esc190/runs/l6c7og3x</a><br/>\n            "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
      },
      "metadata": {},
-     "output_type": "display_data"
+     "output_type": "execute_result"
     },
     {
      "name": "stdout",
      "output_type": "stream",
-     "text": "Train for 8498 steps\nEpoch 1/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.4075 - accuracy: 0.5190\nEpoch 00001: saving model to 32vec.5.weights.01.h5\n8498/8498 [==============================] - 2478s 292ms/step - loss: 0.4075 - accuracy: 0.5190\nEpoch 2/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.5259\nEpoch 00002: saving model to 32vec.5.weights.02.h5\n8498/8498 [==============================] - 2362s 278ms/step - loss: 0.3949 - accuracy: 0.5259\nEpoch 3/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.5304\nEpoch 00003: saving model to 32vec.5.weights.03.h5\n8498/8498 [==============================] - 2356s 277ms/step - loss: 0.3877 - accuracy: 0.5304\nEpoch 4/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.5357\nEpoch 00004: saving model to 32vec.5.weights.04.h5\n8498/8498 [==============================] - 2365s 278ms/step - loss: 0.3809 - accuracy: 0.5357\nEpoch 5/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.5396\nEpoch 00005: saving model to 32vec.5.weights.05.h5\n8498/8498 [==============================] - 2356s 277ms/step - loss: 0.3756 - accuracy: 0.5396\nEpoch 6/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.5443\nEpoch 00006: saving model to 32vec.5.weights.06.h5\n8498/8498 [==============================] - 2361s 278ms/step - loss: 0.3694 - accuracy: 0.5443\nEpoch 7/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.5485\nEpoch 00007: saving model to 32vec.5.weights.07.h5\n8498/8498 [==============================] - 2362s 278ms/step - loss: 0.3644 - accuracy: 0.5485\nEpoch 8/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.5527\nEpoch 00008: saving model to 32vec.5.weights.08.h5\n8498/8498 [==============================] - 2354s 277ms/step - loss: 0.3597 - accuracy: 0.5527\n"
+     "text": [
+      "Train for 8498 steps\nEpoch 1/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.4075 - accuracy: 0.5190\nEpoch 00001: saving model to 32vec.5.weights.01.h5\n8498/8498 [==============================] - 2478s 292ms/step - loss: 0.4075 - accuracy: 0.5190\nEpoch 2/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3949 - accuracy: 0.5259\nEpoch 00002: saving model to 32vec.5.weights.02.h5\n8498/8498 [==============================] - 2362s 278ms/step - loss: 0.3949 - accuracy: 0.5259\nEpoch 3/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3877 - accuracy: 0.5304\nEpoch 00003: saving model to 32vec.5.weights.03.h5\n8498/8498 [==============================] - 2356s 277ms/step - loss: 0.3877 - accuracy: 0.5304\nEpoch 4/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.5357\nEpoch 00004: saving model to 32vec.5.weights.04.h5\n8498/8498 [==============================] - 2365s 278ms/step - loss: 0.3809 - accuracy: 0.5357\nEpoch 5/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.5396\nEpoch 00005: saving model to 32vec.5.weights.05.h5\n8498/8498 [==============================] - 2356s 277ms/step - loss: 0.3756 - accuracy: 0.5396\nEpoch 6/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.5443\nEpoch 00006: saving model to 32vec.5.weights.06.h5\n8498/8498 [==============================] - 2361s 278ms/step - loss: 0.3694 - accuracy: 0.5443\nEpoch 7/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.5485\nEpoch 00007: saving model to 32vec.5.weights.07.h5\n8498/8498 [==============================] - 2362s 278ms/step - loss: 0.3644 - accuracy: 0.5485\nEpoch 8/8\n8497/8498 [============================>.] - ETA: 0s - loss: 0.3597 - accuracy: 0.5527\nEpoch 00008: saving model to 32vec.5.weights.08.h5\n8498/8498 [==============================] - 2354s 277ms/step - loss: 0.3597 - accuracy: 0.5527\n"
+     ]
     },
     {
      "data": {
-      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x24340e51c48>"
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x24340e51c48>"
+      ]
      },
      "execution_count": 17,
      "metadata": {},
@@ -298,5 +277,28 @@
    "outputs": [],
    "source": []
   }
- ]
+ ],
+ "metadata": {
+  "language_info": {
+   "name": "python",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "version": "3.7.4-final"
+  },
+  "orig_nbformat": 2,
+  "file_extension": ".py",
+  "mimetype": "text/x-python",
+  "name": "python",
+  "npconvert_exporter": "python",
+  "pygments_lexer": "ipython3",
+  "version": 3,
+  "kernelspec": {
+   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
+   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
 }
\ No newline at end of file
diff --git a/universalSentenceEmbedding.ipynb b/universalSentenceEmbedding.ipynb
index 1a8c1ab..139bf70 100644
--- a/universalSentenceEmbedding.ipynb
+++ b/universalSentenceEmbedding.ipynb
@@ -7,7 +7,8 @@
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
-   }
+   },
+   "version": "3.7.4-final"
   },
   "orig_nbformat": 2,
   "file_extension": ".py",
@@ -15,14 +16,32 @@
   "name": "python",
   "npconvert_exporter": "python",
   "pygments_lexer": "ipython3",
-  "version": 3
+  "version": 3,
+  "kernelspec": {
+   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
+   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
+  }
  },
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 5,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": "Successfully logged in to Weights & Biases!\nwandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\DS/.netrc\n"
+    },
+    {
+     "output_type": "display_data",
+     "data": {
+      "text/plain": "<IPython.core.display.HTML object>",
+      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190\" target=\"_blank\">https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190/runs/0gacffn5\" target=\"_blank\">https://app.wandb.ai/epicrunze/USE%2Bdecoder-esc190/runs/0gacffn5</a><br/>\n            "
+     },
+     "metadata": {}
+    }
+   ],
    "source": [
     "import numpy as np \n",
     "import tensorflow as tf \n",
@@ -35,15 +54,22 @@
     "!wandb login \"41c25b4fc8e96d4ae0d96e0abd4d69787a6ea35f\"\n",
     "import wandb\n",
     "from wandb.keras import WandbCallback\n",
-    "wandb.init(project=\"lstm-autoencoder-esc190\")\n",
-    "config = wandb.config\n"
+    "wandb.init(project=\"USE+decoder-esc190\")\n",
+    "config = wandb.config\n",
+    ""
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 6,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": "Size of vocabulary: 117659\nLongest definition (words): 54\nNumber of definitions: 135959\nSize of definition vocabulary: 46948\n(135959, 54)\n"
+    }
+   ],
    "source": [
     "data_dir = \"data_wordnet\"\n",
     "data = utils.read_dir(data_dir)\n",
@@ -60,10 +86,73 @@
     "\n",
     "def_vectors = utils.convert_word2int(definitions, word2num)\n",
     "\n",
-    "x_train = utils.defs_to_np(def_vectors, max_length)\n",
+    "y_train = utils.defs_to_np(def_vectors, max_length)\n",
+    "\n",
+    "print(y_train.shape)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tensorflow_hub as hub\n",
+    "#creating embeddings from definitions\n",
+    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#calculating embeddings, in tensor format\n",
+    "embeddings = embed(utils.defs2str(definitions))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 512)]             0         \n_________________________________________________________________\nrepeat_vector_1 (RepeatVecto (None, 54, 512)           0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 54, 32)            69760     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 54, 64)            24832     \n_________________________________________________________________\ntime_distributed (TimeDistri (None, 54, 100)           6500      \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 54, 46948)         4741748   \n=================================================================\nTotal params: 4,842,840\nTrainable params: 4,842,840\nNon-trainable params: 0\n_________________________________________________________________\n"
+    }
+   ],
+   "source": [
+    "#creating decoder model, taking in embedded strings, and calculating a resultant from them\n",
+    "\n",
+    "from tensorflow.keras.models import Model, Sequential\n",
+    "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, RepeatVector, TimeDistributed, Lambda\n",
+    "from tensorflow.keras.optimizers import Adam\n",
+    "from tensorflow.keras.losses import categorical_crossentropy\n",
     "\n",
-    "print(x_train.shape)"
+    "\n",
+    "inputlayer = Input(shape=(512,))\n",
+    "repeatlayer = RepeatVector(max_length)(inputlayer)\n",
+    "decodingLSTM1 = LSTM(32, return_sequences=True)(repeatlayer)\n",
+    "decodingLSTM2 = LSTM(64, return_sequences=True)(decodingLSTM1)\n",
+    "denseboi = TimeDistributed(Dense(100, activation=\"relu\"))(decodingLSTM2)\n",
+    "finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(denseboi)\n",
+    "output = finalDense\n",
+    "\n",
+    "model = Model(inputs=inputlayer, outputs=output)\n",
+    "optimizer = Adam(learning_rate = 0.0003)\n",
+    "model.compile(loss = categorical_crossentropy, optimizer = optimizer, metrics = [\"accuracy\"])\n",
+    "\n",
+    "model.summary()"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
   }
  ]
 }
\ No newline at end of file
diff --git a/utils.py b/utils.py
index dc83b9e..432a17b 100644
--- a/utils.py
+++ b/utils.py
@@ -1,5 +1,6 @@
 import os
 import numpy as np
+import tensorflow as tf
 
 #useful dicts and stuff
 pos_dict = {"a": "adj", "r": "adv", "n": "noun", "v": "verb"}
@@ -116,8 +117,16 @@ def defs_to_np(definitions, max_length, padding_num = 0):
     #return np.reshape(output, (output.shape + (1,))
     return output
 
+def defs2str(definitions):
+    '''converts a list of list of words to a list of strings'''
+    return [" ".join(each) for each in definitions]
 
-
+def to_one_hot(x):
+    vocab_size = 46948 + 1
+    #output = tf.zeros((x.shape)+(vocab_size,))
+    #mask = np.array(x) > 0
+    label = tf.one_hot(x, vocab_size)
+    return x, label[:, 1:]
 
 
 
