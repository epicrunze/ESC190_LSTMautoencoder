{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitprojenvvenv5183e90ff64f49ee83b8a29c549cc789",
   "display_name": "Python 3.7.4 64-bit ('projenv': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import os\n",
    "import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Size of vocabulary: 117659\nLongest definition (words): 54\nNumber of definitions: 135959\nSize of definition vocabulary: 46948\n(135959, 54)\n"
    }
   ],
   "source": [
    "data_dir = \"data_wordnet\"\n",
    "data = utils.read_dir(data_dir)\n",
    "definitions, max_length = utils.get_definitions(data)\n",
    "\n",
    "print(\"Size of vocabulary: {}\".format(len(data)))\n",
    "print(\"Longest definition (words): {}\".format(max_length))\n",
    "print(\"Number of definitions: {}\".format(len(definitions)))\n",
    "\n",
    "word2num, num2word = utils.get_word_dicts(definitions)\n",
    "vocab_size = len(list(word2num.keys()))\n",
    "\n",
    "print(\"Size of definition vocabulary: {}\".format(vocab_size))\n",
    "\n",
    "def_vectors = utils.convert_word2int(definitions, word2num)\n",
    "\n",
    "x_train = utils.defs_to_np(def_vectors, max_length)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, RepeatVector, TimeDistributed, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "'''\n",
    "class LSTMAutoencoder(Model):\n",
    "    def __init__(self, vocab_size, max_length):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        #self.inputy = Input(shape=(None,), dtype=\"int32\")\n",
    "        self.embedding = Embedding(input_dim=50000, output_dim=64, input_length=max_length, mask_zero=True)\n",
    "        self.encodingLSTM1 = LSTM(32, return_sequences=True)\n",
    "        self.encodingLSTM2 = LSTM(16)\n",
    "        self.repeatlayer = RepeatVector(max_length)\n",
    "        self.decodingLSTM1 = LSTM(16, return_sequences=True)\n",
    "        self.decodingLSTM2 = LSTM(32, return_sequences=True)\n",
    "        self.denseboi = TimeDistributed(Dense(100, activation=\"relu\"))\n",
    "        self.finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #x = self.inputy(inputs)\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        mask = self.embedding.compute_mask(inputs)\n",
    "        x = self.encodingLSTM1(x, mask=mask)\n",
    "        x = self.encodingLSTM2(x, mask=mask)\n",
    "        x = self.repeatlayer(x)\n",
    "        x = self.decodingLSTM1(x, mask=mask)\n",
    "        x = self.decodingLSTM2(x, mask=mask)\n",
    "        x = self.denseboi(x)\n",
    "        x = self.finalDense(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "#training embedding layer\n",
    "'''\n",
    "embedding = Sequential()\n",
    "embedding.add(Embedding(input_dim=vocab_size+1, output_dim=64, input_length=max_length, mask_zero=True))\n",
    "\n",
    "\n",
    "inputs = Input(shape=(None,))\n",
    "embedding = Embedding(input_dim=vocab_size+1, output_dim=64, input_length=max_length, mask_zero=True)(inputs)\n",
    "mask = Embedding(input_dim=50000, output_dim=64, input_length=max_length, mask_zero=True).compute_mask(inputs)\n",
    "encodingLSTM1 = LSTM(32, return_sequences=True)(embedding, mask=mask)\n",
    "encodingLSTM2 = LSTM(16)(encodingLSTM1, mask=mask)\n",
    "repeatlayer = RepeatVector(max_length)(encodingLSTM2)\n",
    "decodingLSTM1 = LSTM(16, return_sequences=True)(repeatlayer)\n",
    "decodingLSTM2 = LSTM(32, return_sequences=True)(decodingLSTM1, mask=mask)\n",
    "denseboi = TimeDistributed(Dense(100, activation=\"relu\"))(decodingLSTM2)\n",
    "finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(denseboi)\n",
    "output = finalDense\n",
    "'''\n",
    "inputs = Input(shape=(None,))\n",
    "embedding = Embedding(input_dim=vocab_size+1, output_dim=64, input_length=max_length, mask_zero=True)(inputs)\n",
    "mask = Embedding(input_dim=50000, output_dim=64, input_length=max_length, mask_zero=True).compute_mask(inputs)\n",
    "encodingLSTM1 = LSTM(64, return_sequences=True)(embedding, mask=mask)\n",
    "encodingLSTM2 = LSTM(32)(encodingLSTM1, mask=mask)\n",
    "repeatlayer = RepeatVector(max_length)(encodingLSTM2)\n",
    "decodingLSTM1 = LSTM(32, return_sequences=True)(repeatlayer, mask=mask)\n",
    "decodingLSTM2 = LSTM(64, return_sequences=True)(decodingLSTM1, mask=mask)\n",
    "denseboi = TimeDistributed(Dense(100, activation=\"relu\"))(decodingLSTM2)\n",
    "finalDense = TimeDistributed(Dense(vocab_size, activation=\"softmax\"))(denseboi)\n",
    "output = finalDense\n",
    "#creating optimizers and loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 64)     3004736     input_2[0][0]                    \n__________________________________________________________________________________________________\ntf_op_layer_NotEqual (TensorFlo [(None, None)]       0           input_2[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, None, 64)     33024       embedding[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 32)           12416       lstm[0][0]                       \n__________________________________________________________________________________________________\nrepeat_vector (RepeatVector)    (None, 54, 32)       0           lstm_1[0][0]                     \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   (None, 54, 32)       8320        repeat_vector[0][0]              \n__________________________________________________________________________________________________\nlstm_3 (LSTM)                   (None, 54, 64)       24832       lstm_2[0][0]                     \n__________________________________________________________________________________________________\ntime_distributed (TimeDistribut (None, 54, 100)      6500        lstm_3[0][0]                     \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 54, 46948)    4741748     time_distributed[0][0]           \n==================================================================================================\nTotal params: 7,831,576\nTrainable params: 7,831,576\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=output)\n",
    "optimizer = Adam(learning_rate = 0.0003)\n",
    "model.compile(loss = categorical_crossentropy, optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "\n",
    "#embedding.compile(\"rmsprop\", \"mse\")\n",
    "\n",
    "model.summary()\n",
    "#embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(x):\n",
    "    vocab_size = 46948 + 1\n",
    "    #output = tf.zeros((x.shape)+(vocab_size,))\n",
    "    #mask = np.array(x) > 0\n",
    "    label = tf.one_hot(x, vocab_size)\n",
    "    return x, label[:, 1:]\n",
    "\n",
    "def predict_embeddings(x):\n",
    "    x = embedding.predict(x, steps=54)\n",
    "    return x, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"int32\")\n",
    "#np.random.shuffle(x_train)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "dataset = dataset.map(to_one_hot)\n",
    "dataset = dataset.shuffle(1000).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"32vec.5.weights.08.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train for 100 steps\n"
    }
   ],
   "source": [
    "#model.fit(dataset, epochs=1, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testkek = x_train[:30]\n",
    "output = model.predict(testkek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(30, 54, 46948)\n"
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(30, 54)\n"
    }
   ],
   "source": [
    "words = tf.argmax(output, axis=-1) + 1\n",
    "print(words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "97\n"
    }
   ],
   "source": [
    "print(word2num[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "definition:  having the necessary means or skill or know-how or authority to do something \n\nencoded definition:  having the fact place or tendency or behavior or or or or something \n\ndefinition:  not having the necessary means or skill or know-how \n\nencoded definition:  not having the qualities force or accepted or rules \n\ndefinition:  facing away from the axis of an organ or organism \n\nencoded definition:  inheritance added from the axis of an organ or organism \n\ndefinition:  nearest to or facing toward the axis of an organ or organism \n\nencoded definition:  censorship to or depending toward the end of an organ or sounds \n\ndefinition:  facing or on the side toward the apex \n\nencoded definition:  facing or on the side during the apex \n\ndefinition:  facing or on the side toward the base \n\nencoded definition:  facing or on the side during the base \n\ndefinition:  especially of muscles \n\nencoded definition:  especially of muscles \n\ndefinition:  drawing away from the midline of the body or from an adjacent part \n\nencoded definition:  spend added from the midline of the body of of of maturity body \n\ndefinition:  especially of muscles \n\nencoded definition:  especially of muscles \n\ndefinition:  bringing together or drawing toward the midline of the body or toward an adjacent part \n\nencoded definition:  pull something or shelter at the beginning of the body of or of gene body \n\ndefinition:  being born or beginning \n\nencoded definition:  being engineer or spoils \n\ndefinition:  coming into existence \n\nencoded definition:  coming into existence \n\ndefinition:  bursting open with force as do some ripe seed vessels \n\nencoded definition:  ceramic usually by force as or or shock or groups \n\ndefinition:  giving birth \n\nencoded definition:  giving birth \n\ndefinition:  in or associated with the process of passing from life or ceasing to be \n\nencoded definition:  in or shaped with a process of relation or or the manufacturer to its \n\ndefinition:  being on the point of death \n\nencoded definition:  being on the point of death \n\ndefinition:  breathing your last \n\nencoded definition:  breathing your heart \n\ndefinition:  occurring at the time of death \n\nencoded definition:  occurring at the time of death \n\ndefinition:  shortened by condensing or rewriting \n\nencoded definition:  shortened by Mormons or rewriting \n\ndefinition:  with parts removed \n\nencoded definition:  with both life \n\ndefinition:  abridged to half its original length \n\nencoded definition:  attractiveness to front large life system \n\ndefinition:  summarized or abridged \n\nencoded definition:  plate or abridged \n\ndefinition:  not shortened \n\nencoded definition:  not shortened \n\ndefinition:  complete \n\nencoded definition:  complete \n\ndefinition:  perfect or complete or pure \n\nencoded definition:  acceptance or physically or pure \n\ndefinition:  lacking compromising or mitigating elements \n\nencoded definition:  lacking compromising or heart elements \n\ndefinition:  exact \n\nencoded definition:  exact \n\ndefinition:  being without doubt or reserve \n\nencoded definition:  being without doubt or obvious \n\ndefinition:  total and all-embracing \n\nencoded definition:  remain and seven \n\ndefinition:  absolute \n\nencoded definition:  positively \n\n"
    }
   ],
   "source": [
    "testkekboi = iter(testkek)\n",
    "for sentence in words.numpy().tolist():\n",
    "    size = 0\n",
    "    print(\"definition: \", end =\" \")\n",
    "    for word in next(testkekboi):\n",
    "        if word == 0:\n",
    "            break\n",
    "        size += 1\n",
    "        print(num2word[word], end = \" \")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"encoded definition: \", end =\" \")\n",
    "    encsize = 0\n",
    "    for word in sentence:\n",
    "        if word == 0 or encsize == size:\n",
    "            break\n",
    "        encsize += 1\n",
    "        print(num2word[word], end=\" \")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}